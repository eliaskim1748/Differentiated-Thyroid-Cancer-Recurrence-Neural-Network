#Loading libraries
library(keras)
library(caret)
library(tidyverse)
library(fastDummies)

#Installing TensorFlow and Keras in Python
install_keras()

#Loading data
cancer <- read_csv("Thyroid_Diff.csv")

head(cancer)

#Removing duplicates to prevent redundancy
cancer1 <- unique(cancer)

#Creating dummy columns
cancer2 <- cancer1 |> 
  dummy_cols(remove_first_dummy = TRUE) |> 
  select_if(is.numeric)

#Normalizing Age
min_max <- function(x){
  (x - min(x)) / (max(x) - min(x))
}

cancer2$Age <- min_max(cancer2$Age)

#Creating train and test sets
splitIndex <- createDataPartition(cancer2$Recurred_Yes, p = .8, list = FALSE)

train_data <- as.matrix(cancer2[splitIndex, 1:40])
train_targets <- as.matrix(cancer2[splitIndex, 41])

test_data <- as.matrix(cancer2[-splitIndex, 1:40])
test_targets <- as.matrix(cancer2[-splitIndex, 41])

#Building the model
build_model <- function(){
  model <- keras_model_sequential() |> 
    layer_dense(32, activation = "relu") |>
    layer_dense(32, activation = "relu") |> 
    layer_dense(1, activation = "sigmoid")
  
  model |> compile(optimizer = "rmsprop",
                   loss = "binary_crossentropy",
                   metrics = "accuracy")
  model
}

#Validating with K-fold validation
fold_id <- sample(rep(1:4, length.out = nrow(train_data)))
all_accuracy_histories <- list()
all_loss_histories <- list()

for (i in 1:4) {
  cat("Processing fold #", i, "\n")
  
  val_indices <- which(fold_id == i)
  val_data <- train_data[val_indices, ]
  val_targets <- train_targets[val_indices]
  
  partial_train_data <- train_data[-val_indices, ]
  partial_train_targets <- train_targets[-val_indices]
  
  model <- build_model()
  history <- model |>  fit(
    partial_train_data,
    partial_train_targets,
    validation_data = list(val_data, val_targets),
    epochs = 100,
    batch_size = 16,
    verbose = 0,
  )
  
  loss_history <- history$metrics$val_loss
  all_loss_histories[[i]] <- loss_history
  
  accuracy_history <- history$metrics$val_accuracy
  all_accuracy_histories[[i]] <- accuracy_history
}

#Graphing validation loss and accuracy
##The model starts overfitting around 20 epochs
average_loss_history <- rowMeans(do.call(cbind, all_loss_histories))
plot(average_loss_history, xlab = "epoch", ylab = "Average Validation Loss", type = 'l')

average_accuracy_history <- rowMeans(do.call(cbind, all_accuracy_histories))
plot(average_accuracy_history, xlab = "epoch", ylab = "Average Validation Accuracy", type = 'l')

#Training the final model
model <- build_model()
model |>  fit(train_data, train_targets,
              epochs = 20, 
              batch_size = 16, 
              verbose = 0)

#Predicting on the test data
predictions_prob <- model |>  predict(test_data)

predictions_class <- ifelse(predictions_prob > 0.5, 1, 0)

confusionMatrix(factor(predictions_class), factor(test_targets))

#Clearing the workspace
rm(list = ls())
